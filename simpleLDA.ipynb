{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very simple Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that the documents in a corpus have been generated by a generative model, and tries to work backwards from the documents to find a set of topics that are likely to have generated the corpus.  \n",
    "Documents in the corpus are bags-of-words.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from r code found here: http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparatory stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy to handle the matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a toy corpus. Each document is a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [  'europe brussels brexit britain great again ukip',\n",
    "          'brussels belgium leave europe',\n",
    "          'europe eu britain brexit transport',\n",
    "          'farage eu ukip brussels',\n",
    "          'europe brexit great',\n",
    "          'transport infrastructure cars pollution',\n",
    "          'cars bikes cars',\n",
    "          'cars pollution transport minister bikes',\n",
    "          'bikes infrastructure transport europe',\n",
    "          'pollution cars minister']\n",
    "docs = [doc.split(' ') for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 2 # number of topics\n",
    "alpha = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "beta = 0.001 # hyperparameter\n",
    "iterations = 100 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a word ID to each unique word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordIDs = {}\n",
    "currentID = 0\n",
    "for doc in docs:\n",
    "\tfor i in range(len(doc)):\n",
    "\t\tif doc[i] not in wordIDs:\n",
    "\t\t\twordIDs[doc[i]] = currentID\n",
    "\t\t\tcurrentID +=1\n",
    "vocab = list(range(len(wordIDs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly assign topics to words in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cwt = np.zeros((K, len(vocab))) # initialize word-topic count matrix. wt refers to dimensions W * T\n",
    "ta = [np.zeros((len(doc))) for doc in docs] # initialize topic assignment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in range(len(docs)): # for each document\n",
    "\tfor w in range(len(docs[d])): # for each token in document d \n",
    "\t\tta[d][w] = np.random.randint(K)\n",
    "\t\tti = int(ta[d][w]) # topic index\n",
    "\t\twi = wordIDs[docs[d][w]] # wordID for token w\n",
    "\t\tCwt[ti,wi] +=1 # update word-topic matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word-topic count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cdt = np.zeros((len(docs), K)) # Document-topic matrix. dt refers to dimensions D * T\n",
    "for d in range(len(docs)): # for each document d\n",
    "\tfor t in range(K): # for each topic K\n",
    "\t\tfor thing in ta[d]:\n",
    "\t\t\tif t == thing:\n",
    "\t\t\t\tCdt[d,t] += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this random assignment, we already have (poor, random) <b>topic representations</b> of both all the <b>documents</b> and <b>word distributions of all the topics</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collapsed Gibbs sampling happens here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(iterations): # for each iteration\n",
    "\tfor d in range(len(docs)): # for each document\n",
    "\t\tfor w in range(len(docs[d])): # for each token\n",
    "\t\t\tt0 = int(ta[d][w]) # initial topic assignment to token w\n",
    "\t\t\twid = wordIDs[docs[d][w]] # wordID of token w\n",
    "\n",
    "\t\t\tCdt[d][t0] = Cdt[d][t0]-1 # don't include token w in our document-topic count matrix when sampling for token w\n",
    "\t\t\tCwt[t0,wid] = Cwt[t0,wid]-1 # don't include token w in our word-topic count matrix when sampling for token w\n",
    "\n",
    "            ## UPDATE TOPIC ASSIGNMENT FOR EACH WORD -- COLLAPSED GIBBS SAMPLING.\n",
    "\t\t\tdenominator_a = sum(Cdt[d]) + K * alpha # number of tokens in document + number topics * alpha\n",
    "\t\t\tdenominator_b = np.sum(Cwt, axis=1) + len(vocab) * beta # number of tokens in each topic + # of words in vocab * beta\n",
    "\n",
    "\t\t\tp_z = (Cwt[:,wid] + beta) / denominator_b * (Cdt[d,:] + alpha) / denominator_a # calculating probability word belongs to each topic\n",
    "\t\t\tt1 = np.random.choice(range(K), size=1, replace=True, p=p_z/sum(p_z)) # draw topic for word n from multinomial using probabilities calculated above\n",
    "\n",
    "\n",
    "\t\t\tta[d][w] = t1 # update topic assignment list with newly sampled topic for token w.\n",
    "\t\t\tCdt[d,t1] = Cdt[d,t1]+1 # re-increment document-topic matrix with new topic assignment for token w.\n",
    "\t\t\tCwt[t1,wid] = Cwt[t1,wid]+1 # re-increment word-topic matrix with new topic assignment for token w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for the probabilty:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(z_i = j\\ |\\ \\textbf{z}_{-i}, w_i, d_i \\*) = \\propto $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  0.  0.  0.  0.  1.  2.  0.  0.  2.  4.  0.  2.  5.  3.  3.  0.]\n",
      " [ 0.  3.  3.  2.  2.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  2.]] \n",
      "\n",
      " [array([ 0.,  1.,  1.,  1.,  1.,  0.,  0.]), array([ 1.,  1.,  1.,  0.]), array([ 0.,  0.,  1.,  1.,  0.]), array([ 1.,  0.,  0.,  1.]), array([ 0.,  1.,  1.]), array([ 0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.]), array([ 0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.])] \n",
      "\n",
      " [[ 3.  4.]\n",
      " [ 1.  3.]\n",
      " [ 3.  2.]\n",
      " [ 2.  2.]\n",
      " [ 1.  2.]\n",
      " [ 4.  0.]\n",
      " [ 3.  0.]\n",
      " [ 4.  1.]\n",
      " [ 4.  0.]\n",
      " [ 2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(Cwt, '\\n\\n', ta, '\\n\\n', Cdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse generated topics.  \n",
    "Parameter <code>theta</code> normalizes the counts in the document-topic count matrix <code>Cdt</code> to compute the probability that a document belongs to each topic.  \n",
    "<code>theta</code> is the <b>posterior mean</b> of the parameter <code>θ</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33333333  0.66666667]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.28571429  0.71428571]\n",
      " [ 0.66666667  0.33333333]\n",
      " [ 0.4         0.6       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.4         0.6       ]\n",
      " [ 0.57142857  0.42857143]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.6         0.4       ]]\n"
     ]
    }
   ],
   "source": [
    "theta = (Cdt + alpha) / np.reshape(np.sum(Cdt + alpha, axis=1),(len(docs), 1)) # topic probabilities per document\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>phi</code> normalizes the word-topic count matrix <code>Cwt</code> to compute the probabilities of words given topics.  \n",
    "<code>phi</code> is  the <b>posterior mean</b> of the parameter <code>φ</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.37902650e-05   7.13945853e-02   2.37902650e-05   2.37902650e-05\n",
      "    4.76043203e-02   2.38140553e-02   2.37902650e-05   2.38140553e-02\n",
      "    2.37902650e-05   4.76043203e-02   9.51848504e-02   2.37902650e-05\n",
      "    4.76043203e-02   2.37902650e-05   7.13945853e-02   7.13945853e-02\n",
      "    2.37902650e-05]\n",
      " [  1.18975115e-01   2.37902650e-05   7.13945853e-02   4.76043203e-02\n",
      "    2.37902650e-05   2.37902650e-05   4.76043203e-02   2.37902650e-05\n",
      "    2.38140553e-02   2.37902650e-05   2.37902650e-05   2.38140553e-02\n",
      "    2.37902650e-05   1.18975115e-01   2.37902650e-05   2.37902650e-05\n",
      "    4.76043203e-02]]\n"
     ]
    }
   ],
   "source": [
    "phi = (Cwt + beta) / (np.sum(Cwt+beta)) # topic probabilities per word\n",
    "#colnames(phi) = vocab\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
