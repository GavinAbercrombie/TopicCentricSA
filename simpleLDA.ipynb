{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA for Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy to handle the matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a toy corpus. Each document is a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [  'europe brussels brexit britain great again ukip',\n",
    "          'brussels belgium leave europe',\n",
    "          'europe eu britain brexit transport',\n",
    "          'farage eu ukip brussels',\n",
    "          'europe brexit great',\n",
    "          'transport infrastructure cars pollution',\n",
    "          'cars bikes cars',\n",
    "          'cars pollution transport minister bikes',\n",
    "          'bikes infrastructure transport europe',\n",
    "          'pollution cars minister']\n",
    "docs = [doc.split(' ') for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 2 # number of topics\n",
    "alpha = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "beta = 0.001 # hyperparameter\n",
    "iterations = 100 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a word ID to each unique word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordIDs = {}\n",
    "currentID = 0\n",
    "for doc in docs:\n",
    "\tfor i in range(len(doc)):\n",
    "\t\tif doc[i] not in wordIDs:\n",
    "\t\t\twordIDs[doc[i]] = currentID\n",
    "\t\t\tcurrentID +=1\n",
    "vocab = list(range(len(wordIDs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly assign topics to words in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cwt = np.zeros((K, len(vocab))) # initialize word-topic count matrix. wt refers to dimensions W * T\n",
    "ta = [np.zeros((len(doc))) for doc in docs] # initialize topic assignment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in range(len(docs)): # for each document\n",
    "\tfor w in range(len(docs[d])): # for each token in document d \n",
    "\t\tta[d][w] = np.random.randint(K)\n",
    "\t\tti = int(ta[d][w]) # topic index\n",
    "\t\twi = wordIDs[docs[d][w]] # wordID for token w\n",
    "\t\tCwt[ti,wi] +=1 # update word-topic matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word-topic count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cdt = np.zeros((len(docs), K)) # Document-topic matrix. dt refers to dimensions D * T\n",
    "for d in range(len(docs)): # for each document d\n",
    "\tfor t in range(K): # for each topic K\n",
    "\t\tfor thing in ta[d]:\n",
    "\t\t\tif t == thing:\n",
    "\t\t\t\tCdt[d,t] += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(iterations): # for each pass through the corpus\n",
    "\tfor d in range(len(docs)): # for each document\n",
    "\t\tfor w in range(len(docs[d])): # for each token\n",
    "\t\t\tt0 = int(ta[d][w]) # initial topic assignment to token w\n",
    "\t\t\twid = wordIDs[docs[d][w]] # wordID of token w\n",
    "\n",
    "\t\t\tCdt[d][t0] = Cdt[d][t0]-1 # we don't want to include token w in our document-topic count matrix when sampling for token w\n",
    "\t\t\tCwt[t0,wid] = Cwt[t0,wid]-1 # we don't want to include token w in our word-topic count matrix when sampling for token w\n",
    "\n",
    "            ## UPDATE TOPIC ASSIGNMENT FOR EACH WORD -- COLLAPSED GIBBS SAMPLING MAGIC.  Where the magic happens.\n",
    "\t\t\tdenom_a = sum(Cdt[d]) + K * alpha # number of tokens in document + number topics * alpha\n",
    "\t\t\tdenom_b = np.sum(Cwt, axis=1) + len(vocab) * beta # number of tokens in each topic + # of words in vocab * beta\n",
    "\n",
    "\t\t\tp_z = (Cwt[:,wid] + beta) / denom_b * (Cdt[d,:] + alpha) / denom_a # calculating probability word belongs to each topic\n",
    "\t\t\tt1 = np.random.choice(range(K), size=1, replace=True, p=p_z/sum(p_z)) # draw topic for word n from multinomial using probabilities calculated above\n",
    "\n",
    "\n",
    "\t\t\tta[d][w] = t1 # update topic assignment list with newly sampled topic for token w.\"\"\"\n",
    "\t\t\tCdt[d,t1] = Cdt[d,t1]+1 # re-increment document-topic matrix with new topic assignment for token w.\n",
    "\t\t\tCwt[t1,wid] = Cwt[t1,wid]+1 # re-increment word-topic matrix with new topic assignment for token w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  5.  3.  3.  2.]\n",
      " [ 5.  0.  0.  2.  2.  1.  2.  1.  1.  2.  4.  0.  2.  0.  0.  0.  0.]] \n",
      "\n",
      " [array([ 1.,  0.,  0.,  1.,  1.,  1.,  1.]), array([ 0.,  1.,  1.,  1.]), array([ 1.,  1.,  1.,  0.,  1.]), array([ 0.,  1.,  1.,  0.]), array([ 1.,  0.,  1.]), array([ 1.,  1.,  0.,  0.]), array([ 0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.]), array([ 0.,  1.,  1.,  1.]), array([ 0.,  0.,  0.])] \n",
      "\n",
      " [[ 2.  5.]\n",
      " [ 1.  3.]\n",
      " [ 1.  4.]\n",
      " [ 2.  2.]\n",
      " [ 1.  2.]\n",
      " [ 2.  2.]\n",
      " [ 3.  0.]\n",
      " [ 4.  1.]\n",
      " [ 1.  3.]\n",
      " [ 3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Cwt, '\\n\\n', ta, '\\n\\n', Cdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
